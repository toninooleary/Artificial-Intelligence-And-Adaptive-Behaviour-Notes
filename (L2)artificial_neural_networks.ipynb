{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic old fasioned ai:\n",
    "- decompose the world into symbols and manipulate them to produce actions.\n",
    "\n",
    "diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic AI:\n",
    "- Manipulating categories/symbols to create an output.\n",
    "\n",
    "- eg.g a patient has characteristics, they would be manipulated to produce a diagnosis.\n",
    "\n",
    "diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectionism\n",
    "\n",
    "- Connectionism looks at how the brain works at the neural level, specifically how people learn and remember.\n",
    "\n",
    "- The brain itself doesn't work in the way that symbolic AI does. Thousands of neurons represent symbols(E.g. thousands of neurons to represent a chair)\n",
    "\n",
    "- Therefore, AI doesn't come from manipulation of symbols but distributed processes and the interaction of neurons.\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward Neural Networks\n",
    "\n",
    "- The simplest type of neural network. Signals travel only one way: from input to output.\n",
    "    - They can easily be written manually\n",
    "\n",
    "![diagram](./pictures/feed_forward.PNG \"Dipiction of a feed forward neural network\")\n",
    "\n",
    "* nodes (neurons)\n",
    "* Edges, connections (synapses and axons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input layer\n",
    "\n",
    "* An image matrix would be unrolled to create the first layer of the network.\n",
    "\n",
    "![diagram](./pictures/feed_forward_input.PNG \"Visual representation of how inputs of a feed forward work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "* The output will give a value that represents a classification.\n",
    "\n",
    "* there can be more than one output describing features of the input.\n",
    "\n",
    "![diagram](./pictures/feed_forward_output.PNG \"The outputs that the feed forward network might give\")\n",
    "\n",
    "* In this case, the example feed forward network gave two outputs one to represent how quare it is and one to represent how dark it is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing layer\n",
    "\n",
    "It consists of:\n",
    "- Activation (calculating the weights sum)\n",
    "- Transfer (step functions)\n",
    "\n",
    "![diagram](./pictures/processing_layer.PNG \"Dipiction of the processing layer\")\n",
    "\n",
    "update image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights:\n",
    "\n",
    "- each edge has a weight (represents the strength of that connection)\n",
    "- All weighted edges will give you a weight matrix\n",
    "\n",
    "![diagram](./pictures/weights.PNG \"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation:\n",
    "\n",
    "activation = Weights $*$ inputs\n",
    "\n",
    "- There are amny ways to do Activation\n",
    "- We will be doing this by taking the linear sum (the product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working out the Activation (product)\n",
    "![diagram](./pictures/activation.PNG \"The mathematics behing activation\")\n",
    "\n",
    "#### Example:\n",
    "![diagram](./pictures/activation_example.PNG \"An example of how to work out activation in practice\")\n",
    "\n",
    "* You could do this with loops but these are slow. The better method is matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication example:\n",
    "\n",
    "#### With one output node:\n",
    "![diagram](./pictures/matrix_multiplication.PNG \"An example of how to work out activation in practice\")\n",
    "\n",
    "#### With two output nodes:\n",
    "![diagram](./pictures/matrix_multiplication_two_nodes.PNG \"An example of how to work out activation in practice\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer:\n",
    "\n",
    "- Gives you the final output for the network\n",
    "\n",
    "### Step function:\n",
    "- Uses a threshhold. \n",
    "    - If the activation > 0.5 output 1\n",
    "    - if its less than 0.5 output 0\n",
    "\n",
    "### Bias:\n",
    "\n",
    "- It allows you to shift the activation function by adding a constant.\n",
    "    - (manipulating the final output by adding a constant)  \n",
    "\n",
    "#### Step with Bias\n",
    "\n",
    "![diagram](./pictures/step_with_bias.PNG \"An example of the step function with a bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other transfer functions:\n",
    "\n",
    "![diagram](./pictures/transfer_functions.PNG \"Different types of transfer function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND gate in neural network\n",
    "\n",
    "![diagram](./pictures/and_gate_nn.PNG \"How the and gate is represented using neural networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperplanes\n",
    "\n",
    "- You can put boundaries in these planes.\n",
    "- The neural network draws a line in this plane. This line is defined by the weights and boundaries.\n",
    "\n",
    "You can do lots of logic gates by shifting weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusive Or:\n",
    "\n",
    "#### linear separability:\n",
    "\n",
    "- networks with only linear activation are restricted in the computations they can perfom.\n",
    "    - This is because can only partition input space with straight lines\n",
    "    - Many problems (e.g. XOR) are not linear separable. \n",
    "\n",
    "You must use a multi-layer neural network for exclusive Or\n",
    "\n",
    "no way to do it in planes with one line:\n",
    "\n",
    "#### XOR problem:\n",
    "![diagram](./pictures/xor_problem.PNG \"XOR gates can't be represented using a single layer perceptron\")\n",
    "\n",
    "#### XOR solution\n",
    "![diagram](./pictures/xor_solved.PNG \"XOR gates can be represented by a multi-layered neural network\")\n",
    "\n",
    "More layers mean more complex classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation algorithm:\n",
    "\n",
    "- Back propagation allows weights to be adjusted by going back through the neural network. The heighest weights will be changed to minimise the error.\n",
    "\n",
    "- The error rate is maasured as:\n",
    "    - $ Error = Output - desired $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search:\n",
    "\n",
    "        While (Error!=0)\n",
    "            w1 = randn\n",
    "            w2 = randn\n",
    "            b = randn\n",
    "            Error = 0\n",
    "            For all input combinations\n",
    "                Error = Error +(Output -Desired)\n",
    "            end\n",
    "        end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}